{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hypotension.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPh0xJNMcrRMvAhekTMLcWx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vitaldb/pyvital/blob/master/hypotension.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qs7dd85cwvlz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e2d61306-c2fb-4e15-ff8a-6b30c920c49a"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "############## 본 프로그램의 옵션들 ##############\n",
        "MINUTES_AHEAD = 1  # 저혈압을 1분 전에 예측\n",
        "LSTM_NODES = 16\n",
        "BATCH_SIZE = 256\n",
        "MAX_CASES = 100  # 본 예제에서 사용할 최대 case 수\n",
        "\n",
        "############## VitalDB 로딩 ##############\n",
        "df_trks = pd.read_csv('https://api.vitaldb.net/v2/trks')  # 트랙 목록\n",
        "df_cases = pd.read_csv(\"https://api.vitaldb.net/cases\")  # 임상 정보\n",
        "\n",
        "############## Case selection ##############\n",
        "caseids = list(\n",
        "    set(df_trks[df_trks['tname'] == 'Solar8000/ART_MBP']['caseid']) & \\\n",
        "    set(df_cases[df_cases['age'] > 18]['caseid']) & \\\n",
        "    set(df_cases[~df_cases['opname'].str.contains(\"transplant\")]['caseid'])\n",
        ")\n",
        "print('Total {} cases found'.format(len(caseids)))\n",
        "np.random.shuffle(caseids)  # caseid를 무작위로 섞음\n",
        "\n",
        "############## dataset 생성 ##############\n",
        "def load_trk(tid, interval=1):\n",
        "    try:\n",
        "        url = 'https://api.vitaldb.net/' + tid\n",
        "        dtmbps = pd.read_csv(url).values\n",
        "    except:\n",
        "        return np.empty(0)\n",
        "    if len(dtmbps) == 0:\n",
        "        return np.empty(0)\n",
        "    dtmbps[:,0] /= interval  # convert time to row\n",
        "    trklen = int(np.nanmax(dtmbps[:,0])) + 1  # find maximum index (array length)\n",
        "    ret = np.full(trklen, np.nan)  # create a dense array\n",
        "    for idx, val in dtmbps:  # copy values\n",
        "        ret[int(idx)] = val\n",
        "    return ret\n",
        "\n",
        "# 최종 생성할 데이터셋\n",
        "x = []  # 각 레코드의 혈압 시계열 데이터\n",
        "y = []  # 각 레코드의 출력값 (3분 후 저혈압 발생 여부)\n",
        "valid_mask = []  # 각 샘플이 유효한지 여부 (유효하지 않은 샘플도 그림을 그리기 위해 추출함)\n",
        "y_caseid = []  # 각 레코드의 caseid\n",
        "\n",
        "# 최대로 로딩할 case 수\n",
        "ncase = min(MAX_CASES, len(caseids))\n",
        "icase = 0\n",
        "for caseid in caseids:\n",
        "    print('loading {} ({}/{})...'.format(caseid, icase + 1, ncase), flush=True, end='')\n",
        "\n",
        "    # vital 파일로부터 로딩\n",
        "    #cmd = \"vital_recs.exe -rl {:05.0f}.vital 2 ABP_MBP\".format(caseid)\n",
        "    #pipe = subprocess.Popen(cmd, stdout=subprocess.PIPE, shell=True)\n",
        "    #mbps = pd.read_csv(io.StringIO(pipe.stdout.read().decode('utf-8')), comment='#').values[:, 1:]\n",
        "\n",
        "    # 현 case의 트랙을 로딩\n",
        "    tid = df_trks[(df_trks['caseid'] == caseid) & (df_trks['tname'] == 'Solar8000/ART_MBP')]['tid'].values[0]\n",
        "    mbps = load_trk(tid, 2)  # 2초 인터벌\n",
        "\n",
        "    # 2시간 이내의 case 들은 사용하지 않음\n",
        "    if len(mbps) < 360:\n",
        "        print('case len < 2h')\n",
        "        continue\n",
        "\n",
        "    # a-line 연결 전이면 제거\n",
        "    with np.errstate(invalid='ignore'):\n",
        "        mbps[mbps < 40] = np.nan\n",
        "\n",
        "    # 처음과 마지막의 결측값을 제거\n",
        "    case_valid_mask = ~np.isnan(mbps)\n",
        "    mbps = mbps[(np.cumsum(case_valid_mask) != 0) & (np.cumsum(case_valid_mask[::-1])[::-1] != 0)]\n",
        "\n",
        "    # 중간 결측값을 직전값으로 대체\n",
        "    mbps = pd.DataFrame(mbps).fillna(method='ffill').values.flatten()\n",
        "    \n",
        "    # event data 뽑음\n",
        "    # 입력 혈압 (20초 = 10개)\n",
        "    # 중간 윈도우 (MINUTES_AHEAD)\n",
        "    # 결과 혈압 (1분 = 30개)\n",
        "    case_sample = 0\n",
        "    case_event = 0\n",
        "    for i in range(len(mbps) - (10 + MINUTES_AHEAD * 30)):  # 주변 30분 최소값을 사용하기 때문에 시작 후 15분 부터 종료 후 15분 까지 사용\n",
        "        segx = mbps[i:i + 10]\n",
        "\n",
        "        # y는 mbp 기준\n",
        "        segy = mbps[i + 10 + MINUTES_AHEAD * 30:i + 10 + MINUTES_AHEAD * 30 + 30]\n",
        "\n",
        "        # 중간에 nan 있으면?\n",
        "        valid = True\n",
        "        if np.any(segx > 150):  # 어떤 샘플이든 150 이상이면?\n",
        "            valid = False\n",
        "        elif np.any(segy > 150):  # 어떤 샘플이든 150 이상이면?\n",
        "            valid = False\n",
        "        elif np.any(np.abs(np.diff(segx)) > 50):  # 2초만에 30 mmHg 이상 변하면\n",
        "            valid = False\n",
        "        elif np.any(np.abs(np.diff(segy)) > 50):  # 2초만에 30 mmHg 이상 변하면\n",
        "            valid = False\n",
        "        elif (segx < 40).all():  # 어떤 샘플이든 40 이하이면?\n",
        "            valid = False\n",
        "        elif (segy < 40).all():  # 어떤 샘플이든 40 이하이면?\n",
        "            valid = False\n",
        "\n",
        "        evt = np.nanmax(segy) < 65\n",
        "        x.append(segx)  # 20초 segment\n",
        "        y.append(evt)  # 최대 값이 65 미만이어야 함\n",
        "        valid_mask.append(valid)\n",
        "        y_caseid.append(caseid)\n",
        "        \n",
        "        if valid:\n",
        "            case_sample += 1\n",
        "            if evt:\n",
        "                case_event += 1\n",
        "\n",
        "    if case_sample > 0:\n",
        "        icase += 1\n",
        "        print(\"{} samples {} events ({:.1f}%)\".format(case_sample, case_event, 100*case_event/case_sample))\n",
        "    else:\n",
        "        print('all nan')\n",
        "\n",
        "    if icase >= ncase:\n",
        "        break\n",
        "\n",
        "# 최종적으로 로딩 된 caseid\n",
        "caseids = np.unique(y_caseid)\n",
        "\n",
        "# 입력 데이터셋을 python array 에서 numpy array로 변경\n",
        "x = np.array(x)[...,None]  # LSTM 에 입력으로 넣으려면 3차원으로 만들어야함\n",
        "y = np.array(y) \n",
        "valid_mask = np.array(valid_mask)\n",
        "y_caseid = np.array(y_caseid)\n",
        "\n",
        "# train, test set 을 case 단위로 나눔\n",
        "ntest = int(ncase * 0.2)\n",
        "ntrain = ncase - ntest\n",
        "caseids_train = caseids[:ntrain]\n",
        "caseids_test = caseids[ncase - ntest:ncase]\n",
        "\n",
        "# train set과 test set 으로 나눔\n",
        "train_mask = np.array([caseid in caseids_train for caseid in y_caseid])\n",
        "test_mask = np.array([caseid in caseids_test for caseid in y_caseid])\n",
        "\n",
        "# test set은 그림을 그려야 하므로 invalid 값까지 전체 데이터도 필요\n",
        "test_x = x[test_mask]\n",
        "test_y = y[test_mask]\n",
        "test_y_caseid = y_caseid[test_mask]\n",
        "\n",
        "# valid 한 값만 포함하는 배열 (학습 및 성능 평가시 사용)\n",
        "train_x_valid = x[train_mask & valid_mask]\n",
        "train_y_valid = y[train_mask & valid_mask]\n",
        "test_x_valid = x[test_mask & valid_mask]\n",
        "test_y_valid = y[test_mask & valid_mask]\n",
        "\n",
        "testname = '{}cases {}ahead batchsize={} total {}, train {} ({} events {:.1f}%), test {} ({} events {:.1f}%)'.format(MAX_CASES, MINUTES_AHEAD, BATCH_SIZE, len(y), len(train_y_valid), sum(train_y_valid), 100*np.mean(train_y_valid), len(test_y_valid), sum(test_y_valid), 100*np.mean(test_y_valid))\n",
        "print(testname)\n",
        "\n",
        "############## 모델 학습 ##############\n",
        "from keras.models import Sequential\n",
        "from keras.models import Model, load_model\n",
        "from keras.layers import Dense, LSTM, Input, BatchNormalization\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from sklearn.metrics import auc, classification_report, confusion_matrix, accuracy_score, roc_curve, roc_auc_score, f1_score, precision_recall_curve\n",
        "import tensorflow as tf\n",
        "\n",
        "# 임시 폴더를 생성\n",
        "tempdir = 'output'\n",
        "if not os.path.exists(tempdir):\n",
        "    os.mkdir(tempdir)\n",
        "weight_path = tempdir + \"/weights.hdf5\"\n",
        "\n",
        "# build a model\n",
        "model = Sequential()\n",
        "model.add(LSTM(LSTM_NODES, input_shape=x.shape[1:]))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', tf.keras.metrics.AUC()])\n",
        "hist = model.fit(train_x_valid, train_y_valid, validation_split=0.1, epochs=100, batch_size=BATCH_SIZE, class_weight={0:1, 1:5},\n",
        "                 callbacks=[ModelCheckpoint(monitor='val_loss', filepath=weight_path, verbose=1, save_best_only=True),\n",
        "                            EarlyStopping(monitor='val_loss', patience=2, verbose=0, mode='auto')])\n",
        "\n",
        "# 최적의 모델을 저장\n",
        "model.load_weights(weight_path)\n",
        "open(tempdir + \"/model.json\", \"wt\").write(model.to_json())\n",
        "\n",
        "############## 결과 출력 ##############\n",
        "# test을 한번에 예측\n",
        "test_y_pred = model.predict(test_x_valid).flatten()\n",
        "\n",
        "precision, recall, thmbps = precision_recall_curve(test_y_valid, test_y_pred)\n",
        "auprc = auc(recall, precision)\n",
        "\n",
        "fpr, tpr, thmbps = roc_curve(test_y_valid, test_y_pred)\n",
        "auroc = auc(fpr, tpr)\n",
        "\n",
        "thval = 0.5\n",
        "f1 = f1_score(test_y_valid, test_y_pred > thval)\n",
        "acc = accuracy_score(test_y_valid, test_y_pred > thval)\n",
        "tn, fp, fn, tp = confusion_matrix(test_y_valid, test_y_pred > thval).ravel()\n",
        "\n",
        "testres = 'auroc={:.3f}, auprc={:.3f} acc={:.3f}, F1={:.3f}, PPV={:.1f}, NPV={:.1f}, TN={}, fp={}, fn={}, TP={}'.format(auroc, auprc, acc, f1, tp/(tp+fp)*100, tn/(tn+fn)*100, tn, fp, fn, tp)\n",
        "print(testres)\n",
        "\n",
        "# 최종 폴더명을 변경\n",
        "odir = testname + ' ' + testres\n",
        "os.rename(tempdir, odir)\n",
        "\n",
        "############## 그래프 그리기 ##############\n",
        "# auroc curve\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.plot(fpr, tpr)\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.savefig('{}/auroc.png'.format(odir))\n",
        "plt.close()\n",
        "\n",
        "# auprc curve\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.plot(recall, precision)\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.savefig('{}/auprc.png'.format(odir))\n",
        "plt.close()\n",
        "\n",
        "# 각 case 그림\n",
        "for caseid in caseids_test:\n",
        "    case_mask = (y_caseid == caseid)\n",
        "    case_len = np.sum(case_mask)\n",
        "    if case_len == 0:\n",
        "        continue\n",
        "\n",
        "    # case 내의 x, y, valid_mask 를 만든다\n",
        "    case_x = x[case_mask]\n",
        "    case_y = y[case_mask]\n",
        "    case_valid_mask = valid_mask[case_mask]\n",
        "    \n",
        "    # case 에러를 구하고 출력\n",
        "    case_predy = model.predict(case_x).flatten()\n",
        "    case_rmse = np.nanmean((case_y - case_predy) ** 2) ** 0.5\n",
        "    print('{}\\t{}\\t'.format(caseid, case_rmse))\n",
        "\n",
        "    # 그림 생성\n",
        "    plt.figure(figsize=(20, 4))\n",
        "    plt.xlim([0, case_len + MINUTES_AHEAD * 30])\n",
        "    t = np.arange(0, case_len)\n",
        "\n",
        "    # 저혈압 상태일 때를 붉은 반투명 배경으로\n",
        "    ax1 = plt.gca()\n",
        "    for i in range(len(case_y)):\n",
        "        if case_y[i]:\n",
        "            ax1.axvspan(i + MINUTES_AHEAD * 30, i + MINUTES_AHEAD * 30 + 1, color='r', alpha=0.1, lw=0)\n",
        "\n",
        "    # 65 mmHg 가로선\n",
        "    ax1.axhline(y=65, color='r', alpha=0.5)\n",
        "    ax1.plot(t + 10, case_x[:,-1], color='r')\n",
        "    ax1.set_ylim([0, 150])\n",
        "\n",
        "    ax2 = ax1.twinx()\n",
        "    \n",
        "    # valid 한 샘플만 그린다\n",
        "    case_predy[~case_valid_mask] = np.nan\n",
        "    ax2.plot(t, case_predy)\n",
        "    ax2.set_ylim([0, 1])\n",
        "    \n",
        "    # 그림 저장\n",
        "    plt.savefig('{}/{:.3f}_{}.png'.format(odir, case_rmse, caseid))\n",
        "    plt.close()\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total 3452 cases found\n",
            "loading 3967 (1/100)...6736 samples 126 events (1.9%)\n",
            "loading 1915 (2/100)...2230 samples 388 events (17.4%)\n",
            "loading 1872 (3/100)...4033 samples 0 events (0.0%)\n",
            "loading 4212 (4/100)...9065 samples 0 events (0.0%)\n",
            "loading 5135 (5/100)...11116 samples 184 events (1.7%)\n",
            "loading 4778 (6/100)...4636 samples 514 events (11.1%)\n",
            "loading 3364 (7/100)...10050 samples 508 events (5.1%)\n",
            "loading 4711 (8/100)...5963 samples 160 events (2.7%)\n",
            "loading 427 (9/100)...4488 samples 0 events (0.0%)\n",
            "loading 1051 (10/100)...4629 samples 0 events (0.0%)\n",
            "loading 4262 (11/100)...2302 samples 0 events (0.0%)\n",
            "loading 4314 (12/100)...5186 samples 18 events (0.3%)\n",
            "loading 4308 (13/100)...3156 samples 35 events (1.1%)\n",
            "loading 2427 (14/100)...4434 samples 34 events (0.8%)\n",
            "loading 758 (15/100)...3469 samples 740 events (21.3%)\n",
            "loading 1503 (16/100)...2305 samples 0 events (0.0%)\n",
            "loading 6323 (17/100)...all nan\n",
            "loading 3564 (17/100)...3334 samples 0 events (0.0%)\n",
            "loading 56 (18/100)...13707 samples 1122 events (8.2%)\n",
            "loading 2044 (19/100)...6725 samples 181 events (2.7%)\n",
            "loading 1599 (20/100)...8980 samples 764 events (8.5%)\n",
            "loading 617 (21/100)...5048 samples 424 events (8.4%)\n",
            "loading 5283 (22/100)...4507 samples 187 events (4.1%)\n",
            "loading 5279 (23/100)...all nan\n",
            "loading 4590 (23/100)...7358 samples 469 events (6.4%)\n",
            "loading 5895 (24/100)...9574 samples 277 events (2.9%)\n",
            "loading 673 (25/100)...4446 samples 112 events (2.5%)\n",
            "loading 3453 (26/100)...4262 samples 2 events (0.0%)\n",
            "loading 115 (27/100)...all nan\n",
            "loading 4700 (27/100)...6380 samples 0 events (0.0%)\n",
            "loading 5165 (28/100)...all nan\n",
            "loading 2359 (28/100)...9036 samples 411 events (4.5%)\n",
            "loading 2192 (29/100)...8642 samples 0 events (0.0%)\n",
            "loading 5194 (30/100)...5919 samples 383 events (6.5%)\n",
            "loading 185 (31/100)...5429 samples 217 events (4.0%)\n",
            "loading 3098 (32/100)...6367 samples 0 events (0.0%)\n",
            "loading 4845 (33/100)...all nan\n",
            "loading 1039 (33/100)...5526 samples 960 events (17.4%)\n",
            "loading 4216 (34/100)...9403 samples 0 events (0.0%)\n",
            "loading 354 (35/100)...6473 samples 574 events (8.9%)\n",
            "loading 96 (36/100)...17893 samples 743 events (4.2%)\n",
            "loading 559 (37/100)...8665 samples 318 events (3.7%)\n",
            "loading 2450 (38/100)...4050 samples 0 events (0.0%)\n",
            "loading 4009 (39/100)...3742 samples 103 events (2.8%)\n",
            "loading 4769 (40/100)...5235 samples 0 events (0.0%)\n",
            "loading 4406 (41/100)...4281 samples 33 events (0.8%)\n",
            "loading 367 (42/100)...6861 samples 56 events (0.8%)\n",
            "loading 1090 (43/100)...5473 samples 0 events (0.0%)\n",
            "loading 3048 (44/100)...4350 samples 0 events (0.0%)\n",
            "loading 4550 (45/100)...5993 samples 53 events (0.9%)\n",
            "loading 2576 (46/100)...2271 samples 0 events (0.0%)\n",
            "loading 6015 (47/100)...2942 samples 107 events (3.6%)\n",
            "loading 468 (48/100)...9297 samples 318 events (3.4%)\n",
            "loading 1938 (49/100)...10254 samples 281 events (2.7%)\n",
            "loading 755 (50/100)...3752 samples 56 events (1.5%)\n",
            "loading 3369 (51/100)...6364 samples 1 events (0.0%)\n",
            "loading 5255 (52/100)...5109 samples 0 events (0.0%)\n",
            "loading 3116 (53/100)...3928 samples 612 events (15.6%)\n",
            "loading 2671 (54/100)...6317 samples 546 events (8.6%)\n",
            "loading 659 (55/100)...6211 samples 26 events (0.4%)\n",
            "loading 2542 (56/100)...3837 samples 15 events (0.4%)\n",
            "loading 5218 (57/100)...1316 samples 0 events (0.0%)\n",
            "loading 2107 (58/100)...4242 samples 0 events (0.0%)\n",
            "loading 2533 (59/100)...2472 samples 261 events (10.6%)\n",
            "loading 283 (60/100)...6360 samples 75 events (1.2%)\n",
            "loading 939 (61/100)...4094 samples 71 events (1.7%)\n",
            "loading 4076 (62/100)...8110 samples 532 events (6.6%)\n",
            "loading 4746 (63/100)...12585 samples 359 events (2.9%)\n",
            "loading 5729 (64/100)...9624 samples 0 events (0.0%)\n",
            "loading 3752 (65/100)...10746 samples 78 events (0.7%)\n",
            "loading 1799 (66/100)...11251 samples 191 events (1.7%)\n",
            "loading 707 (67/100)...all nan\n",
            "loading 499 (67/100)...9648 samples 237 events (2.5%)\n",
            "loading 5130 (68/100)...250 samples 0 events (0.0%)\n",
            "loading 269 (69/100)...4901 samples 22 events (0.4%)\n",
            "loading 2654 (70/100)...7580 samples 308 events (4.1%)\n",
            "loading 5861 (71/100)...2251 samples 0 events (0.0%)\n",
            "loading 4390 (72/100)...5432 samples 210 events (3.9%)\n",
            "loading 1438 (73/100)...2347 samples 808 events (34.4%)\n",
            "loading 5100 (74/100)...8726 samples 3030 events (34.7%)\n",
            "loading 2244 (75/100)...4356 samples 40 events (0.9%)\n",
            "loading 4999 (76/100)...11318 samples 434 events (3.8%)\n",
            "loading 1976 (77/100)...12327 samples 126 events (1.0%)\n",
            "loading 5474 (78/100)...2995 samples 0 events (0.0%)\n",
            "loading 5692 (79/100)...4213 samples 8 events (0.2%)\n",
            "loading 5292 (80/100)...2449 samples 52 events (2.1%)\n",
            "loading 3235 (81/100)...2717 samples 215 events (7.9%)\n",
            "loading 530 (82/100)...5217 samples 124 events (2.4%)\n",
            "loading 6154 (83/100)...2261 samples 150 events (6.6%)\n",
            "loading 2066 (84/100)...6563 samples 239 events (3.6%)\n",
            "loading 1913 (85/100)...6947 samples 126 events (1.8%)\n",
            "loading 4742 (86/100)...7228 samples 0 events (0.0%)\n",
            "loading 4404 (87/100)...1274 samples 1 events (0.1%)\n",
            "loading 868 (88/100)...2523 samples 523 events (20.7%)\n",
            "loading 1944 (89/100)...7300 samples 227 events (3.1%)\n",
            "loading 4768 (90/100)...4269 samples 63 events (1.5%)\n",
            "loading 743 (91/100)...9911 samples 317 events (3.2%)\n",
            "loading 584 (92/100)...8948 samples 171 events (1.9%)\n",
            "loading 2212 (93/100)...all nan\n",
            "loading 5059 (93/100)...8441 samples 1241 events (14.7%)\n",
            "loading 2442 (94/100)...1442 samples 54 events (3.7%)\n",
            "loading 3828 (95/100)...6994 samples 0 events (0.0%)\n",
            "loading 1854 (96/100)...6989 samples 356 events (5.1%)\n",
            "loading 2814 (97/100)...4909 samples 243 events (5.0%)\n",
            "loading 3015 (98/100)...1642 samples 1 events (0.1%)\n",
            "loading 3816 (99/100)...2667 samples 0 events (0.0%)\n",
            "loading 3686 (100/100)...7984 samples 2468 events (30.9%)\n",
            "100cases 1ahead batchsize=256 total 629783, train 490407 (18789 events 3.8%), test 107151 (6630 events 6.2%)\n",
            "Epoch 1/100\n",
            "1720/1725 [============================>.] - ETA: 0s - loss: 0.2384 - accuracy: 0.9475 - auc: 0.9535\n",
            "Epoch 00001: val_loss improved from inf to 0.18574, saving model to output/weights.hdf5\n",
            "1725/1725 [==============================] - 14s 8ms/step - loss: 0.2382 - accuracy: 0.9475 - auc: 0.9536 - val_loss: 0.1857 - val_accuracy: 0.9248 - val_auc: 0.9725\n",
            "Epoch 2/100\n",
            "1720/1725 [============================>.] - ETA: 0s - loss: 0.1925 - accuracy: 0.9563 - auc: 0.9648\n",
            "Epoch 00002: val_loss improved from 0.18574 to 0.10778, saving model to output/weights.hdf5\n",
            "1725/1725 [==============================] - 14s 8ms/step - loss: 0.1925 - accuracy: 0.9563 - auc: 0.9648 - val_loss: 0.1078 - val_accuracy: 0.9587 - val_auc: 0.9711\n",
            "Epoch 3/100\n",
            "1719/1725 [============================>.] - ETA: 0s - loss: 0.1906 - accuracy: 0.9555 - auc: 0.9649\n",
            "Epoch 00003: val_loss improved from 0.10778 to 0.10178, saving model to output/weights.hdf5\n",
            "1725/1725 [==============================] - 13s 8ms/step - loss: 0.1906 - accuracy: 0.9555 - auc: 0.9649 - val_loss: 0.1018 - val_accuracy: 0.9557 - val_auc: 0.9702\n",
            "Epoch 4/100\n",
            "1722/1725 [============================>.] - ETA: 0s - loss: 0.1897 - accuracy: 0.9551 - auc: 0.9649\n",
            "Epoch 00004: val_loss did not improve from 0.10178\n",
            "1725/1725 [==============================] - 13s 8ms/step - loss: 0.1897 - accuracy: 0.9551 - auc: 0.9649 - val_loss: 0.1126 - val_accuracy: 0.9558 - val_auc: 0.9718\n",
            "Epoch 5/100\n",
            "1724/1725 [============================>.] - ETA: 0s - loss: 0.1888 - accuracy: 0.9548 - auc: 0.9650\n",
            "Epoch 00005: val_loss did not improve from 0.10178\n",
            "1725/1725 [==============================] - 13s 8ms/step - loss: 0.1888 - accuracy: 0.9548 - auc: 0.9650 - val_loss: 0.1590 - val_accuracy: 0.9399 - val_auc: 0.9721\n",
            "auroc=0.957, auprc=0.541 acc=0.944, F1=0.499, PPV=55.6, NPV=96.4, TN=98122, fp=2399, fn=3631, TP=2999\n",
            "4768\t0.15453528443326325\t\n",
            "4769\t0.03972815546988648\t\n",
            "4778\t0.32501581181062844\t\n",
            "4999\t0.15089995968786843\t\n",
            "5059\t0.27548747693052256\t\n",
            "5100\t0.42510935063820715\t\n",
            "5130\t0.11837940849118354\t\n",
            "5135\t0.14181291000875193\t\n",
            "5194\t0.17328172703428796\t\n",
            "5218\t0.0016909589595876873\t\n",
            "5255\t0.00405540872680759\t\n",
            "5283\t0.1457371472581422\t\n",
            "5292\t0.14014252042557204\t\n",
            "5474\t0.01972170078803471\t\n",
            "5692\t0.09722242315116435\t\n",
            "5729\t0.02112491863474187\t\n",
            "5861\t0.05991223381110391\t\n",
            "5895\t0.13234314308117415\t\n",
            "6015\t0.1810173292413203\t\n",
            "6154\t0.16278311316869695\t\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}